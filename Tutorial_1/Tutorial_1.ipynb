{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwLcwg0DwMfZ"
      },
      "source": [
        "# COMP541 - LAB #1\n",
        "\n",
        "In this exercise, you’re supposed to preprocess and use the Boston Housing Dataset to perform linear regression and train a model that predicts the house prices given the attributes of an house.\n",
        "\n",
        "The housing dataset has housing related information for 506 neighborhoods in Boston from 1978. Each neighborhood is represented using 13 attributes such as crime rate or distance to employment centers. The goal is to predict the median value of the houses given in $1000's."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyjEwc-hwMfc"
      },
      "source": [
        "## Exercise 0\n",
        "\n",
        "In order to use some necessary functions, we need to import some modules and/or procedures,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KQXJgFfwMfc"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlretrieve\n",
        "from urllib.error import HTTPError\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjQJHEoGwMfd"
      },
      "source": [
        "**urlretrieve** procedure allows us to download the data file. **NumPy** is a popular package which adds fast array/matrix operations to Python via its array structure **np.array**. NumPy also contains modules and functions for statistics (e.g. **np.mean**, **np.std**) and pseduo-random number generation (e.g. **np.random.randint**, **np.random.default_rng**)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYjG5qrvwMfd"
      },
      "source": [
        "First download, and then read the file. You need to download the data within Jupyter notebook (please have a look: **urlretrieve**, **open** functions of Python by typing e.g. **?urlretrieve**). If you look at the data, you see that each house is represented with 13 attributes separated by whitespaces and there are 506 lines in total. Here’s the [link](https://raw.githubusercontent.com/ilkerkesen/ufldl-tutorial/master/ex1/housing.data) to the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofDomG9WwMfe"
      },
      "outputs": [],
      "source": [
        "def download_and_read(data_url):\n",
        "    \"\"\"\n",
        "    download_and_read(data_url)\n",
        "\n",
        "    First download the `data_url`, then read it into an array.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        file_path, http_message = urlretrieve(data_url)\n",
        "    except HTTPError:  # e.g. URL doesn't exist\n",
        "        return None\n",
        "\n",
        "    data = np.loadtxt(file_path)\n",
        "    return data\n",
        "\n",
        "housing_data = download_and_read(\"https://raw.githubusercontent.com/ilkerkesen/ufldl-tutorial/master/ex1/housing.data\")\n",
        "\n",
        "print(\"Testing download_and_read function.\")\n",
        "assert housing_data.shape == (506, 14)\n",
        "assert np.isclose(housing_data.mean(), 66.67816985036704)\n",
        "print('All tests passed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G03nqiOqwMfe"
      },
      "source": [
        "## Exercise 1\n",
        "\n",
        "The resulting data matrix should have 506 rows representing neighborhoods and 14 columns representing the attributes. The last attribute is the median house price to be predicted, so let’s separate it. We will use NumPy’s array indexing operation to split the data array into input x and output y. (Hint: you may want to **np.reshape** y array into a matrix with size 506x1, use reshape procedure for this purpose) Note that, in this tutorial we are going to store instances in the rows, since the popular Python deep learning frameworks like PyTorch and TensorFlow do the same thing. So, it will change our matrix operations' order from **w * x** to **x * w**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-1qF-7QwMff"
      },
      "outputs": [],
      "source": [
        "def split_inputs_outputs(data):\n",
        "    \"\"\"\n",
        "    split_inputs_outputs(data)\n",
        "\n",
        "    Split it into two parts as inputs and outputs along the second axis.\n",
        "    Second split is size of 1. Return both splits as 2-dimensional arrays.\n",
        "    \"\"\"\n",
        "    # TODO: Use python array indexing/slicing operations to seperate the data into x and y matrices.\n",
        "    # reshape y into 506,1 for easier processing later.\n",
        "\n",
        "    # data: 506x14\n",
        "    # x should be 506x13\n",
        "    # y should be 506x1\n",
        "\n",
        "    return x,y\n",
        "\n",
        "\n",
        "x,y = split_inputs_outputs(housing_data)\n",
        "\n",
        "print(\"Testing split_inputs_outputs function.\")\n",
        "assert x.shape == (506, 13)\n",
        "assert y.shape == (506, 1)\n",
        "assert np.isclose(x[0,:].mean(), 62.37687076923077)\n",
        "assert y[122, 0] == 20.5\n",
        "print('All tests passed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbRq8F9nwMff"
      },
      "source": [
        "## Exercise 3\n",
        "As you can see, input attributes have different ranges. We need to normalize attributes by subtracting their mean and then dividing by their standard deviation (hint: take means and standard deviations of column vectors). The mean and std functions calculate mean and standard deviation values of x. Calculate mean and standard deviation values. Perform normalization on input data. You can use mean() and std() functions of Numpy:\n",
        "Ex. usage: mean = x.mean(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7Qtp52owMff"
      },
      "outputs": [],
      "source": [
        "def normalize(x):\n",
        "    \"\"\"\n",
        "        normalize(x)\n",
        "\n",
        "    Take mean and standart deviation of `x` along the first axis. Subtract the mean from x and\n",
        "    divide by standart deviation. Return the result.\n",
        "    \"\"\"\n",
        "    # TODO: Calculate the mean and standard deviation of x\n",
        "    # then normalize it by substracting the mean, and dividing by std.\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "x = normalize(x)\n",
        "\n",
        "print(\"Testing normalize function\")\n",
        "assert x.shape == (506, 13)\n",
        "assert np.isclose(x[122, :].mean(), -0.01051380557556349)\n",
        "print(\"All tests passed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03UjHg4rwMff"
      },
      "source": [
        "### *Important Note on Random Number Generation*\n",
        "Before generating random numbers, strings etc., you need to set a seed, because NumPy uses a pseudo random number generator. In pseudo random number generators you set a seed and you obtain some certain random number generation order based on that seed. If you don’t set a seed, the results you obtain in the next exercises will be different. When you fail in some part, run the cells again starting from the cell or line we set random seed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0mwaNU9wMfg"
      },
      "source": [
        "## Exercise 4\n",
        "It is necessary to split our dataset into training and test subsets so we can estimate how good our model will perform on unseen data. There are 506 house in our dataset. Let’s take 400 of them randomly, use them as training data. Let the rest be test data. In the end, you will have 4 different arrays: xtrn, ytrn, xtst and ytst.\n",
        "\n",
        "We will use **randperm** function to split our dataset into train and test sets. Note that, results will differ since usage of **randperm** function introduces randomness. If you want to overcome this randomness, set a seed by using **Random.seed!** function. In this exercise, we set seed as 1 just before **randperm** call and you need to take the first 400 random samples -not the last 400- as your training data, so that you will get exactly the same results. Use **@doc** macro to see documentation about **randperm** and **Random.seed!** (e.g. type **@doc randperm** to Julia REPL or notebook)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F47R2DtmwMfg"
      },
      "outputs": [],
      "source": [
        "def train_test_split(x, y, split_size=400):\n",
        "    \"\"\"\n",
        "    train_test_split(x, y, split_size=400)\n",
        "\n",
        "    Shuffle both `x` and `y` with same random permutation so that they correspond to each other\n",
        "    on their second axis. Split both into two by `split_size`, return the splits.\n",
        "    \"\"\"\n",
        "    num_examples = y.shape[0]\n",
        "    rand_indexes = np.random.permutation(num_examples)\n",
        "\n",
        "    # TODO: Partition the data into training and test sets. You can simply use python array indexing/slicing operations to do so.\n",
        "    # First shuffle the data by using rand_indexes (you can use it as an index to shuffle the given array, python allows indexing by lists.)\n",
        "    # Then take the first \"split_size\" of the array as the training data, rest as the test data.\n",
        "\n",
        "    return xtrn, xtst, ytrn, ytst\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "xtrn, xtst, ytrn, ytst = train_test_split(x, y)\n",
        "\n",
        "print(\"Testing train_test_split function.\")\n",
        "assert xtrn.shape == (400, 13)\n",
        "assert xtst.shape == (106, 13)\n",
        "assert ytrn.shape == (400, 1)\n",
        "assert ytst.shape == (106, 1)\n",
        "assert np.isclose(xtrn[122, :].mean(), 0.8119304085501324)\n",
        "assert np.isclose(xtst[41, :].mean(), -0.2661499897093574)\n",
        "assert ytrn[122, 0] == 8.8\n",
        "assert ytst[41, 0] == 36.5\n",
        "print(\"All tests passed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCuJKJmswMfg"
      },
      "source": [
        "## Exercise 5\n",
        "Our data is ready to be used. Let’s look at how good a randomly initialized linear regression model performs on our processed data.\n",
        "\n",
        "Basically, we need to use some weights with whom we’re going to multiply the attributes of houses so that we can predict the price of that house. Neighborhoods are represented with 13 attributes and we need to predict the prices which is a single number. We need to have a weight matrices with size of 13x1. We also use a bias value which is 0.\n",
        "\n",
        "To create weight matrix, we will sample from normal distribution with zero mean and a small standard deviation. In this tutorial, our standard deviation value is equal to 0.1. Use np.random.randn function to create a random weight matrix whose values are sampled from a unit normal distribution (mean=0, standard deviation=1). Multiply our weight matrix by 0.1 which is our desired standard deviation. We will not use bias in this part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TNRb4eDwMfg"
      },
      "outputs": [],
      "source": [
        "def create_matrix(x=1, y=13, scale=0.1):\n",
        "    \"\"\"\n",
        "    create_matrix(x=1, y=13, scale=0.1)\n",
        "\n",
        "    Return a matrix of size (`x`, `y`) scaled by `scale`.\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: create a weight matrix w with np.random.randn, that has a shape of (13,1), then multiply it with the scale\n",
        "\n",
        "    return w\n",
        "\n",
        "\n",
        "np.random.seed(1)\n",
        "w = create_matrix()\n",
        "\n",
        "print(\"Testing create_matrix method.\")\n",
        "assert w.shape == (13, 1)\n",
        "assert np.isclose(w[3, 0], -0.10729686221561706)\n",
        "print(\"All tests passed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd-BaxQkwMfg"
      },
      "source": [
        "Note that if you used any other operation that uses a seed after you used **np.random.permutation** function once in the previous exercise, your weight array will not be the same with the example. Please reset your seed and try again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jOoPTbbwMfh"
      },
      "source": [
        "## Exercise 6\n",
        "Now, we have input and weights. Let’s write a function to predict price. Implement the function takes weight matrix and neighborhood attributes as input and outputs a single value, house price prediction. Simply perform a matrix multiplication inside this function and return the output vector. You should use **@** operator to perform a matrix multiplication using NumPy arrays, or you can also use **np.matmul** function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-7DuvKywMfh"
      },
      "outputs": [],
      "source": [
        "def predict(w, x):\n",
        "    \"\"\"\n",
        "    predict(w, x)\n",
        "\n",
        "    Return the dot product of `w` and `x`.\n",
        "    \"\"\"\n",
        "    # TODO: Return the dot product of x and w.\n",
        "\n",
        "ypred = predict(w, xtrn)\n",
        "\n",
        "print(\"Testing predict function\")\n",
        "assert ypred.shape == (400, 1)\n",
        "assert np.isclose(ypred[122, 0], 2.659817742702329)\n",
        "print(\"All tests passed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbsAZ4XQwMfh"
      },
      "source": [
        "ypred is an 400x1 dimensional array/matrix. Each value in this array is the model’s price prediction for an average house in corresponding neighborhood."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-T0AjAetwMfh"
      },
      "source": [
        "## Exercise 7\n",
        "Let’s implement a loss function which is called as Mean Squared Error (MSE),\n",
        "![](https://github.com/OsmanMutlu/rawtext/raw/master/img/Comp541-Lab1-Screenshot7.png)\n",
        "In this function we calculate J, our loss value, average of squared difference between real price values and predicted price values.\n",
        "\n",
        "Implement MSE loss function which takes weight matrix, input matrix (xtrn or xtst) and ground truth prices (ytrn or ytst). Use the **predict** function you implemented above. Helpful functions/attributes: np.sum, np.mean, array.shape, np.abs. You don’t have to use all of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbNRuex3wMfh"
      },
      "outputs": [],
      "source": [
        "def mse_loss(w, x, y):\n",
        "    \"\"\"\n",
        "    mse_loss(w, x, y)\n",
        "\n",
        "    Predict `x` using `w`. Calculate the loss of the predictions using `y`.\n",
        "    \"\"\"\n",
        "    # TODO: Implement the MSE loss.\n",
        "    # 1) use the model to get the prediction\n",
        "    # 2) compare the ground truth (y) with the prediction and calculate the MSE loss.\n",
        "\n",
        "\n",
        "\n",
        "train_loss = mse_loss(w, xtrn, ytrn)\n",
        "test_loss = mse_loss(w, xtst, ytst)\n",
        "\n",
        "print(\"Testing Loss\")\n",
        "assert np.isclose(train_loss, 302.36332258653687)\n",
        "assert np.isclose(test_loss, 294.5122295907839)\n",
        "print(\"All tests passed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2ZC5-4_wMfh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOqa76fywRz4"
      },
      "source": [
        "## Exercise 8 - Closed Form Solution\n",
        "\n",
        "**Normal Equation**\n",
        "\n",
        "Gradient Descent is an iterative algorithm meaning that you need to take multiple steps to get to the Global optimum (to find the optimal parameters) but it turns out that for the special case of **Linear Regression**, there is a way to solve for the optimal values of the parameter theta to just jump in one step to the Global optimum without needing to use an iterative algorithm and this algorithm is called the Normal Equation. It works **only** for Linear Regression and not any other algorithm.\n",
        "\n",
        "Normal Equation is the Closed-form solution for the Linear Regression algorithm which means that we can obtain the optimal parameters by just using a formula that includes a few matrix multiplications and inversions.\n",
        "\n",
        "To calculate the optimal weight matrix W , we take the partial derivative of the MSE loss function with respect to W and set it equal to zero. Then, do a little bit of linear algebra to get the value of W.\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "source: https://www.kaggle.com/code/aminizahra/linear-regression-closed-form\n",
        "\n",
        "Implement a function that calculates the optimal W matrix for our problem, and test out linear regression model with these parameters to assess the results. You may use numpy.transpose() and numpy.linalg.inv() to get the transposed and inverted matrices respectively.\n",
        "\n",
        "For additional information, you can check the following:\n",
        "https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote08.html\n",
        "\n",
        "https://www.kaggle.com/code/aminizahra/linear-regression-closed-form\n",
        "\n",
        "https://medium.com/@ilgyrd/ordinary-least-squares-closed-form-solution-the-dart-way-d7c0ee0e0d02"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def closed_form_solution(x, y):\n",
        "    \"\"\"\n",
        "    closed_form_solution(x, y)\n",
        "    Calculate the optimal W matrix using the closed form solution formula for linear regression, using 'x' and 'y'\n",
        "    \"\"\"\n",
        "    # Implement and return the closed form solution weights\n",
        "\n",
        "\n",
        "\n",
        "optimal_w_trn = closed_form_solution(xtrn, ytrn)\n",
        "\n",
        "train_loss = mse_loss(optimal_w_trn, xtrn, ytrn)\n",
        "test_loss = mse_loss(optimal_w_trn, xtst, ytst)\n",
        "\n",
        "print(\"Testing Loss\")\n",
        "print(train_loss)\n",
        "print(test_loss)\n",
        "assert np.isclose(train_loss, 265.86753884893267)\n",
        "assert np.isclose(test_loss, 264.71798782255473)\n",
        "print(\"All tests passed.\")"
      ],
      "metadata": {
        "id": "WPz1kqjGy9GO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 9: Linear regression with Gradient Descent\n",
        "\n",
        "\n",
        "In this step, we implement **Gradient Descent** to learn the weight matrix w for our linear regression model.\n",
        "\n",
        "The goal is to minimize the **Mean Squared Error (MSE)** loss:\n",
        "\n",
        "$L = (1/n) * Σ (y_i - ŷ_i)² = (1/n) * ||Xw - y||²$\n",
        "\n",
        "The gradient of the loss with respect to the weights is:\n",
        "\n",
        "$∇w L = (2/n) * Xᵀ (Xw - y)$\n",
        "\n",
        "At each iteration, we update the weights using:\n",
        "\n",
        "$w ← w - η ∇w L$\n",
        "\n",
        "where $η$ is the learning rate.\n",
        "We repeat this process for several epochs until the loss stops improving.\n",
        "\n",
        "After training, we’ll compare the results with the **Closed-Form (Normal Equation)** solution to see how close the two approaches are.\n"
      ],
      "metadata": {
        "id": "gIcB5rBYZcr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_mse(w, x, y):\n",
        "    \"\"\"\n",
        "    grad_mse(w, x, y)\n",
        "    Gradient of MSE wrt weights for y_pred = x @ w .\n",
        "    MSE = (1/n) * ||xw - y||^2  -> grad = (2/n) * X^T (Xw - y)\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Implement the gradient of MSE loss.\n",
        "\n",
        "\n",
        "\n",
        "def gradient_descent(x, y, lr=0.05, epochs=3000, w_init=None, tol=1e-9, verbose=False):\n",
        "    \"\"\"\n",
        "    - x: (n, d), y: (n, 1)\n",
        "    - lr: learning rate\n",
        "    - epochs: number of iterations\n",
        "    - w_init: optional (d, 1) init; if None, random ~ N(0, 0.1)\n",
        "    - tol: relative improvement tolerance for early stopping\n",
        "    Returns:\n",
        "      w        -> learned weights (d, 1)\n",
        "      history  -> dict with 'train_loss' curve\n",
        "    \"\"\"\n",
        "    if w_init is None:\n",
        "        np.random.seed(1)\n",
        "        w = create_matrix()\n",
        "    else:\n",
        "        w = w_init.copy()\n",
        "\n",
        "    history = {'train_loss': []}\n",
        "    prev_loss = np.inf\n",
        "\n",
        "    for t in range(epochs):\n",
        "        # forward\n",
        "\n",
        "        # TODO: Forward pass and the loss calculation.\n",
        "        loss =\n",
        "        history['train_loss'].append(loss)\n",
        "\n",
        "\n",
        "        # TODO: backward (gradient calculation) + update\n",
        "        g =\n",
        "        w =\n",
        "\n",
        "        prev_loss = loss\n",
        "\n",
        "        if verbose and (t % 500 == 0 or t == epochs - 1):\n",
        "            print(f\"epoch {t:4d} | train MSE: {loss:.6f}\")\n",
        "\n",
        "    return w, history\n",
        "\n",
        "np.random.seed(1)\n",
        "w0 = create_matrix()\n",
        "w_gd, hist = gradient_descent(xtrn, ytrn, lr=0.05, epochs=4000, w_init=w0, tol=1e-10, verbose=True)\n",
        "\n",
        "gd_train = mse_loss(w_gd, xtrn, ytrn)\n",
        "gd_test  = mse_loss(w_gd, xtst, ytst)\n",
        "\n",
        "cf_train = mse_loss(optimal_w_trn, xtrn, ytrn)\n",
        "cf_test  = mse_loss(optimal_w_trn, xtst, ytst)\n",
        "\n",
        "print(\"== Comparison ==\")\n",
        "print(f\"Closed-form   -> train: {cf_train:.6f} | test: {cf_test:.6f}\")\n",
        "print(f\"Gradient Des. -> train: {gd_train:.6f} | test: {gd_test:.6f}\")\n",
        "print(f\"||w_gd - w_cf||_2: {np.linalg.norm(w_gd - optimal_w_trn):.6f}\")\n",
        "print(f\"Last GD train loss: {hist['train_loss'][-1]:.6f}; epochs used: {len(hist['train_loss'])}\")\n"
      ],
      "metadata": {
        "id": "9JI4IHDv03Eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization: Closed Form Solution vs Gradient Descent"
      ],
      "metadata": {
        "id": "XqyqibXxsz7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# --- Pick one feature to visualize ---\n",
        "cors = [np.corrcoef(xtrn[:, j], ytrn[:, 0])[0, 1] for j in range(xtrn.shape[1])]\n",
        "best_j = int(np.argmax(np.abs(cors)))\n",
        "print(f\"Using feature #{best_j} for visualization\")\n",
        "\n",
        "# --- Prepare test data for that feature ---\n",
        "x_feat = xtst[:, best_j]\n",
        "y_true = ytst[:, 0]\n",
        "\n",
        "# --- Compute predictions for test samples ---\n",
        "y_pred_gd = predict(w_gd, xtst)[:, 0]\n",
        "y_pred_cf = predict(optimal_w_trn, xtst)[:, 0]\n",
        "\n",
        "# Gradient Descent Fit ---\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.scatter(x_feat, y_true, color=\"blue\", alpha=0.6, label=\"True data\")\n",
        "\n",
        "x_line = np.linspace(x_feat.min(), x_feat.max(), 200)\n",
        "X_line = np.zeros((200, xtrn.shape[1]))\n",
        "X_line[:, best_j] = x_line\n",
        "y_line_gd = predict(w_gd, X_line)[:, 0]\n",
        "\n",
        "plt.plot(x_line, y_line_gd, color=\"red\", linewidth=2, label=\"GD Prediction line\")\n",
        "plt.xlabel(f\"Feature #{best_j}\")\n",
        "plt.ylabel(\"Predicted / True Price\")\n",
        "plt.title(\"Linear Regression (Gradient Descent)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- 2️⃣ Closed-Form Fit ---\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.scatter(x_feat, y_true, color=\"blue\", alpha=0.6, label=\"True data\")\n",
        "\n",
        "y_line_cf = predict(optimal_w_trn, X_line)[:, 0]\n",
        "plt.plot(x_line, y_line_cf, color=\"green\", linewidth=2, label=\"Closed-form Prediction line\")\n",
        "\n",
        "plt.xlabel(f\"Feature #{best_j}\")\n",
        "plt.ylabel(\"Predicted / True Price\")\n",
        "plt.title(\"Linear Regression (Closed-Form Solution)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PlBOHrXTeVXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent with a trainable bias term"
      ],
      "metadata": {
        "id": "wSSzLh38nOsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_affine(w, b, x):\n",
        "    \"\"\"\n",
        "    y_hat = x @ w + b\n",
        "    w: (d,1), b: scalar (float), x: (n,d)\n",
        "    returns: (n,1)\n",
        "    \"\"\"\n",
        "    return x @ w + b\n",
        "\n",
        "def mse_loss_affine(w, b, x, y):\n",
        "    \"\"\"\n",
        "    MSE = (1/n) * ||(x @ w + b) - y||^2\n",
        "    \"\"\"\n",
        "    err = predict_affine(w, b, x) - y\n",
        "    return (err**2).mean()\n",
        "\n",
        "def grads_affine(w, b, x, y):\n",
        "    \"\"\"\n",
        "    Gradients of MSE wrt w and b for y_hat = Xw + b\n",
        "    ∇_w = (2/n) X^T (Xw + b - y)\n",
        "    ∇_b = (2/n) sum(Xw + b - y)\n",
        "    \"\"\"\n",
        "    n = x.shape[0]\n",
        "    resid = (x @ w + b) - y          # (n,1)\n",
        "    gw = (2.0 / n) * (x.T @ resid)   # (d,1)\n",
        "    gb = float((2.0 / n) * resid.sum())  # scalar\n",
        "    return gw, gb\n",
        "\n",
        "def gradient_descent_affine(x, y, lr=0.05, epochs=3000, w_init=None, b_init=0.0,\n",
        "                            tol=1e-9, verbose=True):\n",
        "    \"\"\"\n",
        "    Full-batch GD for linear regression with a trainable bias.\n",
        "    Returns: w, b, history\n",
        "    \"\"\"\n",
        "    if w_init is None:\n",
        "        np.random.seed(1)\n",
        "        w = create_matrix()          # (d,1) ~ N(0,0.1)\n",
        "    else:\n",
        "        w = w_init.copy()\n",
        "    b = float(b_init)\n",
        "\n",
        "    history = {'train_loss': []}\n",
        "    prev_loss = np.inf\n",
        "\n",
        "    for t in range(epochs):\n",
        "        loss = mse_loss_affine(w, b, x, y)\n",
        "        history['train_loss'].append(loss)\n",
        "\n",
        "        gw, gb = grads_affine(w, b, x, y)\n",
        "        w -= lr * gw\n",
        "        b -= lr * gb\n",
        "\n",
        "        prev_loss = loss\n",
        "        if verbose and (t % 500 == 0 or t == epochs - 1):\n",
        "            print(f\"epoch {t:4d} | train MSE: {loss:.6f} | b: {b:.4f}\")\n",
        "\n",
        "    return w, b, history\n",
        "\n",
        "# ---- Train and evaluate (GD with bias) ----\n",
        "np.random.seed(1)\n",
        "w0 = create_matrix()\n",
        "b0 = 0.0\n",
        "w_gd, b_gd, hist = gradient_descent_affine(xtrn, ytrn, lr=0.05, epochs=4000,\n",
        "                                           w_init=w0, b_init=b0, tol=1e-10)\n",
        "\n",
        "gd_train = mse_loss_affine(w_gd, b_gd, xtrn, ytrn)\n",
        "gd_test  = mse_loss_affine(w_gd, b_gd, xtst, ytst)\n",
        "print(f\"GD (with bias) -> train MSE: {gd_train:.6f} | test MSE: {gd_test:.6f}\")\n",
        "print(f\"Learned bias b: {b_gd:.4f}\")\n"
      ],
      "metadata": {
        "id": "0HkjbD8agBQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a single feature for 2-D view (most correlated with y on train)\n",
        "cors = [np.corrcoef(xtrn[:, j], ytrn[:, 0])[0, 1] for j in range(xtrn.shape[1])]\n",
        "best_j = int(np.argmax(np.abs(cors)))\n",
        "print(f\"Using feature #{best_j} for visualization\")\n",
        "\n",
        "x_feat = xtst[:, best_j]\n",
        "y_true = ytst[:, 0]\n",
        "\n",
        "# Prediction line: vary only that feature, keep others at mean (≈0 after normalization)\n",
        "x_line = np.linspace(x_feat.min(), x_feat.max(), 200)\n",
        "X_line = np.zeros((200, xtrn.shape[1]))\n",
        "X_line[:, best_j] = x_line\n",
        "y_line = predict_affine(w_gd, b_gd, X_line)[:, 0]\n",
        "\n",
        "# Scatter + line\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.scatter(x_feat, y_true, alpha=0.6, label=\"True data\")\n",
        "plt.plot(x_line, y_line, linewidth=2, label=\"Prediction line (GD + bias)\")\n",
        "plt.xlabel(f\"Feature #{best_j}\")\n",
        "plt.ylabel(\"House Price\")\n",
        "plt.title(\"Linear Regression Fit (Test Data)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "u7GHCSrdjtNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XesNE3UNjwCD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}