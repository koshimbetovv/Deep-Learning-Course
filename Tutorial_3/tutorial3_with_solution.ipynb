{"cells":[{"cell_type":"markdown","metadata":{"id":"rhMykNoSrLNI"},"source":["## Tutorial 3 - Training Neural Networks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mEifC30TrLNJ"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","from torch import optim\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"2G5RDR2ErLNK"},"source":["## Network Initialization\n","Initializing neural network layers are very important for training. The required functions for network initialization are located in `torch.nn.init`.\n","\n","There are a few common initialization methods used:\n","- `torch.nn.init.zeros_(tensor)`: Initializes the tensor to all zeros.\n","- `torch.nn.init.normal_(tensor, mean, std)`: Initializes from the normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$.\n","- `torch.nn.init.xavier_uniform_(tensor, g=1)`: Initializes from the uniform distribution $\\mathcal{U}(-a, a)$, where $a$ is given by the formula $g \\times \\sqrt{\\frac{6}{f_{in} + f_{out}}}$. Here, $g$ is called the gain, $f_{in}$ is the input dimensionality of the weight matrix, and $f_{out}$ is the output dimensionality.\n","- `torch.nn.init.xavier_normal_(tensor, g=1)`: Initializes from the normal distribution $\\mathcal{N}(0, \\sigma^{2})$, where $\\sigma = g \\times \\sqrt{\\frac{2}{f_{in} + f_{out}}}$.\n","\n","By default, PyTorch initializes Linear layers using `kaiming_uniform` initialization. You can read more about this in the below link.\n","\n","All these initializations and other choices can be found at the [PyTorch initialization documentation](https://pytorch.org/docs/stable/nn.init.html)."]},{"cell_type":"markdown","metadata":{"id":"ZHTs8cJorLNL"},"source":["# Dropout\n","\n","Dropout is a regularization technique used to prevent **overfitting** in neural networks.  \n","During training, dropout randomly \"drops\" (sets to zero) a fraction $p$ of the neurons' outputs in a layer at each forward pass.  \n","This forces the network to not rely too heavily on any specific neuron and instead learn more robust and generalizable feature representations.\n","\n","Mathematically, for a given layer output vector **h**, dropout creates a masked version:\n","\n","$h' = m \\odot h, \\quad \\text{where } m_i \\sim \\text{Bernoulli}(1-p)$\n","\n","Here, $m$ is a binary mask sampled independently for each neuron, and $\\odot$ denotes element-wise multiplication.\n","\n","In PyTorch, dropout is implemented using:\n","```python\n","nn.Dropout(p)\n","```\n","where p is the parameter that controls the fraction of masking $p$."]},{"cell_type":"markdown","metadata":{"id":"Rs3imuOXrLNL"},"source":["## Normalization\n","To use the different types of normalization, you can use the following:\n","- `torch.nn.BatchNorm(1d/2d/3d)`: Applies batch normalization on data with the given number of dimensions (excluding batch dim)\n","- `torch.nn.GroupNorm()`: Applies group normalization\n","- `torch.nn.InstanceNorm(1d/2d/3d)`: Applies instance normalization on the data\n","- `torch.nn.LayerNorm`: Applies LayerNorm\n","\n","**Batch Normalization**  \n","Normalizes activations across the batch and feature dimensions. It computes the mean and variance for each feature over the entire batch and rescales values accordingly. This helps the network train faster and acts as a mild regularizer. Works best when batch sizes are not too small.\n","\n","$$\n","y = \\gamma \\times \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n","$$\n","Here, $\\gamma, \\beta$ are parameters learned by the normalization layer, which allows for scaling the mean and variance to whatever is best.\n","\n","**Layer Normalization**  \n","Normalizes all features within a single sample, making it independent of the batch size. It is commonly used in Transformers and RNNs, where batch statistics may not be stable or meaningful. The mean and variance are based on the individual samples in the batch, instead of across the whole batch.\n","\n","**RMS Normalization**\n","While LayerNorm is very effective for many applications, it does suffer from one drawback: the need to calculate the mean and standard deviation of the inputs. Many recent models, especially Transformers (which you will learn about later this semester), use a faster normalization method instead, called RMSNorm.\n","\n","$$\n","y = \\gamma \\times \\frac{x}{\\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2 + \\epsilon}}\n","$$\n","\n","Notice here how we do not require any calculation of the mean or variance, instead just normalizing based on the input itself, and additionally we do not have any shift factor $\\beta$. However, we do keep the scale factor $\\gamma$ which is learned.\n","\n","RMSNorm allows for trading off speed (since we don't compute means or variances) for potential performance (since we don't center the activations around 0, and we don't allow for additional shift factors). Thus, it is a design decision of which to use.\n","\n","**Instance Normalization**  \n","Normalizes each feature map for every sample individually. It’s often used in style transfer and image generation tasks, as it removes instance-specific contrast while preserving the overall style.\n","\n","Instance Normalization uses the same equation as LayerNorm, with the only difference being how the mean and variance is calculated (LayerNorm takes the mean and variance over all elements within a sample, while InstanceNorm handles different channels separately, for example).\n","\n","**Group Normalization**  \n","Splits the feature channels into groups and normalizes within each group for every sample. It’s useful when batch sizes are small or vary across training steps, providing a middle ground between BatchNorm and LayerNorm.\n","\n","Again, the equation is the same, the only change is how the mean and variance is calculated (for example over groups of channels).\n","\n","<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20250114183648606652/What-is-Group-Normalization_.jpg\" width=\"600\">\n","\n","\n","For this tutorial, you should only need to use BatchNorm.\n","\n","To use weight normalization, you will need to use a different approach. If `layer` is a layer that you want to apply weight normalization to, you should use `layer = torch.nn.utils.parametrizations.weight_norm(layer)`. This will always apply weight norm."]},{"cell_type":"markdown","metadata":{"id":"ns7mfEfVrLNL"},"source":["### Exercise 1: Creating a Properly Initialized Network\n","Below is the starter code for a basic MLP. You should implement the following:\n","- `3` linear layers, with a ReLU activation between each layer.\n","- The first layer should have input dim `input_dim` and output dim 64. The second layer should have both input and output dims as 64. The last layer should have input dim 64 and output dim `num_classes`.\n","- Initialize all of the linear layers using xavier uniform initialization. (Note: To access the weights of a layer, you need to use `layer.weight`. To access the bias of a layer, you need to use `layer.bias`).\n","- Initialize all bias terms to all zeros.\n","- Use Batch Norm in between each of the layers (Optional).\n","- Use Dropout in between each layer (Optional)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uctbTAU0rLNM"},"outputs":[],"source":["class MLP(nn.Module):\n","    def __init__(self, input_size, num_classes):\n","        super().__init__()\n","        self.linear1 = nn.Linear(input_size, 64)\n","        self.linear2 = nn.Linear(64, 64)\n","        self.linear3 = nn.Linear(64, num_classes)\n","        self.relu = nn.ReLU()\n","        nn.init.xavier_uniform_(self.linear1.weight)\n","        nn.init.xavier_uniform_(self.linear2.weight)\n","        nn.init.xavier_uniform_(self.linear3.weight)\n","        if self.linear1.bias is not None:\n","            nn.init.zeros_(self.linear1.bias)\n","        if self.linear2.bias is not None:\n","            nn.init.zeros_(self.linear2.bias)\n","        if self.linear3.bias is not None:\n","            nn.init.zeros_(self.linear3.bias)\n","\n","    def forward(self, x):\n","        x = x.view(x.shape[0], -1) # flattening to shape: B * L\n","        x = self.relu(self.linear1(x))\n","        x = self.relu(self.linear2(x))\n","        out = self.linear3(x)\n","\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UaNTQu_nrLNM"},"outputs":[],"source":["input_size = 784\n","num_classes = 10\n","learning_rate = 1e-3\n","batch_size = 8\n","num_epochs = 3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3MV4-homrLNM"},"outputs":[],"source":["my_transforms = transforms.Compose(\n","    [\n","        transforms.ToTensor(),                    # Finally converts PIL image to tensor\n","        #transforms.Resize((36, 36)),             # Resizes (32,32) to (36,36)\n","        #transforms.RandomCrop((32, 32)),         # Takes a random (32,32) crop\n","        #transforms.RandomRotation(degrees=45),   # Perhaps a random rotation from -45 to 45 degrees\n","        #transforms.RandomHorizontalFlip(p=0.5),  # Flips the image horizontally with probability 0.5\n","        #transforms.RandomVerticalFlip(p=0.05),   # Flips image vertically with probability 0.05\n","        #transforms.Normalize((0.5,), (0.5,))     # Normalize\n","    ]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rNsje9MlrLNN"},"outputs":[],"source":["train_dataset = datasets.MNIST(root=\"dataset/\", train=True, transform=my_transforms, download=True)\n","test_dataset = datasets.MNIST(root=\"dataset/\", train=False, transform=my_transforms, download=True)\n","train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"yvy2PFNerLNN"},"source":["## Optimizers and Schedulers\n","As covered in the previous tutorial, the optimizers are located in `torch.optim`. Commonly used optimizers are:\n","- `torch.optim.Adam`\n","- `torch.optim.AdamW`\n","- `torch.optim.SGD`\n","- `torch.optim.RMSprop`\n","\n","You can find the full list of optimizers and their details at `https://pytorch.org/docs/stable/optim.html`.\n","\n","Learning rate schedulers are located in `torch.optim.lr_scheduler`. Commonly used ones are:\n","- `torch.optim.lr_scheduler.LinearLR`: Linearly changes the learning rate according to the number of steps\n","- `torch.optim.lr_scheduler.ExponentialLR`: Apply exponential decay to the learning rate. If `gamma` is the exponential factor, then the current learning rate is given by `curr_lr = start_lr * (gamma)^num_steps`.\n","- `torch.optim.lr_scheduler.CosineAnnealingLR`: Implements cosine annealing. A more complicated scheduler, refer to the paper [SGDR: Stochastic Gradient Descent with Warm Restarts](https://arxiv.org/abs/1608.03983) for more details.\n","- `torch.optim.lr_scheduler.ReduceLROnPlateau`: Monitors a given metric, and reduces the learning rate whenever the metric stops improving.\n","\n","All learning rate schedulers take as input the optimizer, as well as any other hyperparameters. If `scheduler` is the learning rate scheduler, then you can update the learning rate by calling `scheduler.step()`.\n","\n","\n","### Weight Decay\n","\n","Weight decay is a form of **regularization** that helps prevent overfitting by discouraging large weight values.  \n","It works by adding a small penalty proportional to the magnitude of the weights to the loss function during optimization.  \n","This causes the optimizer to slightly shrink the weights on every update, keeping them smaller and more stable.\n","\n","With **weight decay**, an extra term proportional to the current weight value is added to the gradient:\n","$w = w - \\eta \\cdot (\\nabla_w L + \\lambda w)$\n","where $\\lambda$ is the **weight decay coefficient**.\n","\n","\n","To use weight decay, you can use the keyword `weight_decay=` when creating the optimizer.\n","\n","### L1/L2 Regularization\n","To apply some type of regularization to the model, you can use:\n","- `sum(p.pow(2).sum() for p in model.parameters())`: L2 normalization\n","- `sum(p.abs().sum() for p in model.parameters())`: L1 normalization"]},{"cell_type":"markdown","metadata":{"id":"sHHPEk8crLNN"},"source":["### Exercise 2: Optimizers and Schedulers\n","Experiment with different choices of optimizers and learning rate schedulers to see how good the performance can be. Initialize an optimizer here called `optimizer`, and a learning rate scheduler called `scheduler`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EaqkxwM9rLNN"},"outputs":[],"source":["model = MLP(input_size, num_classes).to(device)\n","optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n","scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hflYHFt5rLNO"},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GDnwBUyUrLNO"},"outputs":[],"source":["def check_accuracy(loader, model, device):\n","    \"\"\"\n","    Check accuracy of our trained model given a loader and a model\n","\n","    Parameters:\n","        loader: torch.utils.data.DataLoader\n","            A loader for the dataset you want to check accuracy on\n","        model: nn.Module\n","            The model you want to check accuracy on\n","\n","    Returns:\n","        acc: float\n","            The accuracy of the model on the dataset given by the loader\n","    \"\"\"\n","\n","    num_correct = 0\n","    num_samples = 0\n","    model.eval()\n","\n","    # We don't need to keep track of gradients here so we wrap it in torch.no_grad()\n","    with torch.no_grad():\n","        # Loop through the data\n","        for x, y in loader:\n","\n","            # Move data to device\n","            x = x.to(device=device)\n","            y = y.to(device=device)\n","\n","            # Get to correct shape\n","            x = x.reshape(x.shape[0], -1)\n","\n","            # Forward pass\n","            scores = model(x)\n","            _, predictions = scores.max(1)\n","\n","            # Check how many we got correct\n","            num_correct += (predictions == y).sum()\n","\n","            # Keep track of number of samples\n","            num_samples += predictions.size(0)\n","\n","    model.train()\n","    return num_correct / num_samples"]},{"cell_type":"markdown","metadata":{"id":"aAYqwiOUrLNO"},"source":["### Exercise 3: Training Loop\n","Here the basics of the training loop are provided. Implement the forward and backward passes, optimizer and learning rate scheduler steps."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zLtk1t8prLNO","executionInfo":{"status":"ok","timestamp":1730716331900,"user_tz":-180,"elapsed":76859,"user":{"displayName":"HAKAN CAPUK","userId":"01043227722701086806"}},"outputId":"a0d10d5c-aa63-44eb-960c-7e48b54ad9a3"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 7500/7500 [00:21<00:00, 356.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: [1/3] | Loss: 0.228\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 7500/7500 [00:21<00:00, 342.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: [2/3] | Loss: 0.083\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 7500/7500 [00:21<00:00, 350.00it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: [3/3] | Loss: 0.067\n","Accuracy on training set: 98.03\n","Accuracy on test set: 97.24\n"]}],"source":["model.train()\n","for epoch in range(num_epochs):\n","    losses = []\n","    for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):\n","        # Get data to cuda if possible\n","        data = data.to(device=device)\n","        targets = targets.to(device=device)\n","\n","        # TODO: Add code here\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = criterion(output, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        losses.append(loss.item())\n","\n","        mean_loss = sum(losses) / len(losses)\n","    scheduler.step()\n","    print(f\"Epoch: [{epoch+1}/{num_epochs}] | Loss: {mean_loss:.3f}\")\n","\n","# Check accuracy on training & test to see how good our model\n","print(f\"Accuracy on training set: {check_accuracy(train_loader, model, device)*100:.2f}\")\n","print(f\"Accuracy on test set: {check_accuracy(test_loader, model, device)*100:.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"DehSqRYDrLNO"},"source":["### Exercise 4: Improving Performance\n","Modify the optimizer, learning rate scheduler, type of normalization, dropout percentage, weight initialization, learning rate, and regularization, and see how high you can get the accuracy."]},{"cell_type":"markdown","metadata":{"id":"pzYPh6yCrLNO"},"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"undefined.undefined.undefined"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}