{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhERQjR-agw_"
      },
      "source": [
        "# Tutorial 2: Introduction to PyTorch\n",
        "\n",
        "## Tensors\n",
        "\n",
        "A tensor is PyTorch's way of representing a multi-dimensional array. The data it contains can be located on the CPU or GPU. To use PyTorch, first we need to import the `torch` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFXPHzJFagxE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0zL1ocqagxG"
      },
      "source": [
        "## Creating a Tensor\n",
        "\n",
        "There are multiple ways to create a tensor. It can be created by specifying the data inside, similar to a Numpy array, or it can also be created by converting a numpy array into a tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wo1znbDzagxI"
      },
      "outputs": [],
      "source": [
        "# TODO: create a tensor using the default pytorch way.\n",
        "x =\n",
        "\n",
        "# TODO: create a tensor by converting from numpy.\n",
        "x2 =\n",
        "\n",
        "assert torch.all(x == x2)\n",
        "print(\"x and x2 are equal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kldhKf_NagxK"
      },
      "source": [
        "The most important properties of a tensor are the following:\n",
        "- The tensor's shape: `x.shape`. This is the number of dimensions of the tensor, and how many elements are in each dimension.\n",
        "- The tensor's datatype: `x.dtype`.\n",
        "- The tensor's device: `x.device`. Whether the tensor is currently stored on the cpu or gpu, or another device (ex. Mac M2 chip).\n",
        "- Whether the tensor requires a gradient or not: `x.requires_grad`. Whether PyTorch will automatically calculate gradients with respect to this tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqwatGYOagxL"
      },
      "outputs": [],
      "source": [
        "print(x)\n",
        "print(f\"Shape: {}\")\n",
        "print(f\"Datatype: {}\")\n",
        "print(f\"Device: {}\")\n",
        "print(f\"Requires Grad: {}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSACTLQtagxM"
      },
      "source": [
        "There are many other ways of initializing tensors with specific elements. Here, we list some of them. In most cases, the tensor is created by specifying the desired shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDTkk9oFagxN"
      },
      "outputs": [],
      "source": [
        "# TODO: Create tensors that are: empty, all with zeros, uniform from [0,1], from standard normal dist., all ones, identity matrix, with arange, with linspace\n",
        "x =  # empty: A tensor with no data currently inside. The value it starts with depends on the datatype\n",
        "print(x)\n",
        "x =  # zeros: A tensor with 0s for all elements\n",
        "print(x)\n",
        "x =  # rand: A tensor where every element is uniformly sampled from [0, 1]\n",
        "print(x)\n",
        "x =  # randn: A tensor where every element is sampled from a standard normal distribution\n",
        "print(x)\n",
        "x =  # ones: A tensor with 1s for all elements\n",
        "print(x)\n",
        "x =  # eye: A tensor with 1s on the diagonals and 0s everywhere else. If the shape is a square (ex. 5x5), this produces an identity matrix\n",
        "print(x)\n",
        "x =  # arange: Similar to the python range() function. arange(a, b, c) produces a tensor by taking every c-th element between a and b.\n",
        "print(x)\n",
        "x =  # linspace: Takes `steps` number of steps uniformly between the start and end values\n",
        "print(x)\n",
        "\n",
        "# TODO: \"LIKE\" functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeqswDpqagxP"
      },
      "outputs": [],
      "source": [
        "x = torch.arange(4)\n",
        "print(x.bool()) # boolean True/False\n",
        "print(x.short()) # int16\n",
        "print(x.long()) # int64\n",
        "print(x.half()) # float16\n",
        "print(x.float()) # float32\n",
        "print(x.double()) # float64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuM1jXqhagxQ"
      },
      "source": [
        "## Tensor Shape Manipulation\n",
        "\n",
        "It is often very important to get tensors into a desired shape and number of dimensions. To do this, we need to use various functions.\n",
        "- `x.reshape(new_shape)` converts the shape of `x` to have `new_shape`. This depends on the order of the dimensions. Therefore, if the tensor has shape (10, 20, 30) vs (30, 20, 10), the same reshape can produce different results.\n",
        "- `x.view(new_shape)` does the same thing as `reshape`, except it ensures that the memory is contiguous. This means that the memory is actually moved around into the new shape, instead of just changing its appearance.\n",
        "- `x.permute(i1, ..., ik)` permutes the dimensions `i1, ..., ik` in the order specified.\n",
        "- Other important manipulations include `torch.cat`, `x.unsqueeze`, `x.squeeze`, `x.t()`. You should look these up on the PyTorch documentation https://pytorch.org/docs/stable/torch.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLRrwe-EagxR"
      },
      "source": [
        "### Exercise 1\n",
        "\n",
        "Using the above functions for creating tensors and manipulating tensors, perform the following tasks:\n",
        "1) Create a tensor `x` of shape (5, 5, 3) with elements drawn from a normal distribution\n",
        "2) Change its shape into (3, 25) where the 3 comes from the last dimension `Note: You will need multiple functions to achieve this`\n",
        "3) Change the shape into (1, 3, 25, 1) using `unsqueeze`\n",
        "4) Create another tensor `y` which is a 3x3 identity matrix.\n",
        "5) Combine both `x` and `y` into a single tensor `z`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wD2v4zUSagxR"
      },
      "outputs": [],
      "source": [
        "# Write code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALSv3eo7agxS"
      },
      "source": [
        "## Tensor Mathematics\n",
        "\n",
        "You can use the normal operators `+, -, *, /` on tensors, as long as their shapes are broadcastable. This means that by simply adding new dimensions of size 1, or repeating a tensor N times along a single dimension, the shapes can be made to be the same. Therefore, tensors of shape `(3, 3, 5)` and `(1, 3, 5)` are broadcastable since the second tensor can be repeated `3` times along the 0th dimension. However, tensors of shape `(3, 3, 5)` and `(1, 4, 6)` cannot be broadcast together. Using these operators will perform the operation elementwise.\n",
        "\n",
        "Additionally, comparison operators such as `>, <, ==` will be made elementwise, and require the same shape between both tensors. It will return a tensor of booleans.\n",
        "\n",
        "Some other useful math functions include:\n",
        "- `torch.matmul(x, y)` performs matrix multiplication on tensors `x` and `y`.\n",
        "- `torch.dot(x, y)` calculates the dot product of `x` and `y`.\n",
        "- `torch.bmm(x, y)` performs batch matrix multiplication (given tensors of shape `(B, M, N)` and `(B, N, P)`, returns a tensor of shape `(B, M, P)` obtained by performing matrix multiplication on each matrix contained in dimensions `1` and `2`).\n",
        "- `torch.sum(x, dim=N)` sums the elements of the tensor along a given dimension. By default, the dimension is collapsed. Therefore, summing a tensor of shape `(A, B ,C)` alone dimension 2 will result in a tensor of shape `(A, B)`. To keep the dimension, use the keyword `keepdim=True`. This will produce a tensor of shape `(A, B, 1)`.\n",
        "\n",
        "Other math functions to check in the documentation include `torch.sin, torch.cos, torch.max, torch.min, torch.argmax, torch.argmin, torch.abs, torch.norm, torch.clamp, torch.any, and torch.all`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Laji8892agxS"
      },
      "source": [
        "### Exercise 2: Writing Loss Functions\n",
        "\n",
        "Recall from tutorial `1` that the Mean-Squared Error (MSE) loss function is defined by ![](https://github.com/OsmanMutlu/rawtext/raw/master/img/Comp541-Lab1-Screenshot7.png). Write a function called `mse_loss` which takes `2` tensors as input, `x` and `y`, and calculates the MSE between them. Assume that they have the same shape.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wx4erA52agxS"
      },
      "outputs": [],
      "source": [
        "# Write code here\n",
        "def mse_loss(x, y):\n",
        "\n",
        "\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vzq7K3ZagxT"
      },
      "source": [
        "## Tensor Indexing\n",
        "\n",
        "Indexing of tensors follows the same patterns as with numpy. If `x` is a tensor, you can obtain an element of `x` by doing `x[a1, a2, ..., ak]`. To take all elements in a specific dimension, use `:` instead of a number.\n",
        "\n",
        "To get elements satisfying some condition, you can use `x[some condition involving x]`. For example `x[x < 5]` will return only the elements of `x` which are less than `5`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikPwFOWCagxT"
      },
      "outputs": [],
      "source": [
        "x = torch.arange(10)\n",
        "# todo: show elements of x that are lower then 2 or greater then 6\n",
        "\n",
        "# todo: show elements of x that are even\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3e87BNBagxU"
      },
      "source": [
        "## Neural Network Layers\n",
        "\n",
        "Now that we know how to work with tensors, we can use layers and actually learn something.\n",
        "\n",
        "For now, we will work with `Linear` layers. These are the layers of an MLP/Feedforward Network. To define a linear layer, use the following:\n",
        "    `layer = torch.nn.Linear(num_input_elements, num_output_elements, bias=??)`. Applying this to a tensor is equivalent to calculating `y = x A^T + b` where `A` is a matrix of shape `(num_output_elements, num_input_elements)` and `b` is a vector of shape `(num_output_elements)`. By setting `bias=False` we can disable `b`.\n",
        "\n",
        "Another important layer is `torch.nn.ReLU()`. This layer applies the nonlinear activation function `ReLU` to each element of the input. Recall from linear algebra that the composition of multiple affine transformations is itself an affine transformation. Therefore, without using `ReLU`, applying many Linear layers is equivalent to just applying a single Linear layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F93y3Po5agxU"
      },
      "source": [
        "If we want to combine multiple layers together into a pipeline, we can use `network = torch.nn.Sequential(layer1, layer2, layer3, ..., layerk)`. Then, running `network(x)` is equivalent to applying `layerk(... (layer3(layer2(layer1(x))))...)`.\n",
        "\n",
        "Finally, if we want to create a class containing these layers, the class should extend from `torch.nn.Module`. Inside the constructor of this class, the first thing that MUST be done is calling `super().__init__()`. You will receive an error if you do not do this.\n",
        "\n",
        "When creating a class extending from `torch.nn.Module`, in order to define what the class does, you should implement a function `forward(self, ...)` which takes as input the desired inputs of the network, and outputs the result of the network.\n",
        "\n",
        "Finally, if `network` is an object of some class extending `torch.nn.Module`, we can place it on a GPU by using `network.to(device)` where `device` is either a string specifying the device, or a `torch.device` value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riwJaYhcagxU"
      },
      "source": [
        "### Exercise 3: Creating a MLP Network\n",
        "\n",
        "Create a class called `NN` which will contain a simple network of layers, satisfying the following conditions.\n",
        "- The constructor should take as input two integers: `input_size` and `num_classes`.\n",
        "- There should be two Linear layers. The first layer will take as input `input_size` and output `64`, while the second layer will take as input `64` and output `num_classes`.\n",
        "- In between the two Linear layers, there should be a `ReLU` layer.\n",
        "- The input will have shape (B, M, N). Before providing it as input to the network, you should reshape it to have shape `(B, M * N)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eg46EOKFagxU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsZQNwPEagxV"
      },
      "outputs": [],
      "source": [
        "# Write code here\n",
        "\n",
        "class NN(nn.Module):\n",
        "  def __init__(self, #todo: add parameters here):\n",
        "    super().__init__()\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g78xo2JagxV"
      },
      "source": [
        "## Learning MNIST\n",
        "\n",
        "Now, we will combine everything we have learned so far in order to train a simple network. The goal of the network will be to take as input an image of a number (0-9), and output the number it thinks the image contains.\n",
        "\n",
        "This is an example of a classification problem, where the input is the image, and the output is the class (value) that the image belongs to.\n",
        "\n",
        "### Getting and Setting Up the Data\n",
        "First, in order to train this model, we will need the data. We will use a dataset called MNIST, which contains black and white images of shape 28x28, along with an integer specifying the value contained in the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BsO8iulagxV"
      },
      "outputs": [],
      "source": [
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# If you are using a Macbook with Apple Silicon, and have set up PyTorch for your Mac, comment out the above line and uncomment the below line\n",
        "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dataset = datasets.MNIST(root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = datasets.MNIST(root=\"dataset/\", train=False, transform=transforms.ToTensor(), download=True)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSTJ6FdkagxW"
      },
      "outputs": [],
      "source": [
        "INPUT_SIZE = 784\n",
        "NUM_CLASSES = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWSxTEZ0agxW"
      },
      "source": [
        "### Exercise 4\n",
        "\n",
        "Create an NN object using the NN class defined above, and call it `model`. `input_size` should be `INPUT_SIZE` and `num_classes` should be `NUM_CLASSES`. After creating the model, send it to the device specified above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXTlRrnIagxW"
      },
      "outputs": [],
      "source": [
        "# Write code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_9RG_wsagxW"
      },
      "source": [
        "## Training and Optimizing\n",
        "\n",
        "In order to train the model, we need to use an optimizer. An optimizer is a class which implements Stochastic Gradient Descent (SGD) or one of its variants, and uses it to update the weights of a model. To use normal SGD, you can create an object of class `torch.optim.SGD`.\n",
        "\n",
        "However, we will use an optimizer called `Adam`, which is one of the most popular optimizers used today. This is created with `torch.optim.Adam`.\n",
        "\n",
        "In order to create an optimizer, you will need two things: the parameters of a model, and a learning rate. If `model` is a class extending `torch.nn.Module`, you can call `model.parameters()` to obtain all the parameters of the model.\n",
        "\n",
        "Given an optimizer called `optimizer`, there are two important functions.\n",
        "1) `optimizer.zero_grad()`: This removes all previously calculated gradients. You need to call this each time you want to use the optimizer, BEFORE calculating the gradients.\n",
        "2) `optimizer.step()`: Once the gradients have been calculated, calling this will use the gradients to update all the relevant weights.\n",
        "\n",
        "Finally, we need to calculate the gradients. In order to do this, we MUST have a tensor containing a single value, which represents the current loss. If `loss` is the tensor containing this value, all the gradients can be calculated using `loss.backward()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziT8R0YjagxX"
      },
      "source": [
        "### Exercise 5\n",
        "Create an variable called LR and set it equal to `0.001`. Then, create an Adam optimizer called `optimizer`, provide it the parameters of the `model` object created above, and set its learning rate to be `LR`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L93a6jPJagxX"
      },
      "outputs": [],
      "source": [
        "# Write code here\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snkKcI9nagxX"
      },
      "source": [
        "## Running Training\n",
        "\n",
        "Now, we will train our model on the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VSMM2IzagxX"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 3\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    pbar = tqdm(train_loader)\n",
        "    for idx, (data, targets) in enumerate(pbar):\n",
        "        source = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # todo: obtain scores by passing the input data through the model\n",
        "\n",
        "        # todo 2: obtain the loss by using nn.CrossEntropyLoss\n",
        "\n",
        "        # todo 3: reset the grads of the optimizer, do the backward pass, and perform the optimizer step (w = w - lr * grad(loss))\n",
        "\n",
        "\n",
        "        if idx % 100 == 0:\n",
        "            pbar.set_description(f\"Loss: {loss.item():.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6T_b76_agxY"
      },
      "source": [
        "## Evaluating Performance\n",
        "\n",
        "Finally, once we have a trained model, we need to evaluate its performance. To do so, we will use the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qr4Vh-yagxY"
      },
      "outputs": [],
      "source": [
        "def check_accuracy(loader, model):\n",
        "\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device=device)\n",
        "            y = y.to(device=device)\n",
        "\n",
        "            scores = model(x)\n",
        "            _, predictions = scores.max(1)\n",
        "            num_correct += (predictions == y).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "    return num_correct/num_samples\n",
        "\n",
        "print(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:.2f}\")\n",
        "print(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTaX4xAHagxY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}